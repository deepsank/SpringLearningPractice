ðŸ“š Spring Documentation AI Assistant
Shell-Based Local RAG System using Spring AI + Ollama + PGVector

An interactive CLI-based AI assistant that enables semantic search and question-answering over the official Spring Boot documentation using a fully local Retrieval-Augmented Generation (RAG) architecture.

Built with Spring Boot, Spring AI, Ollama, and PostgreSQL (PGVector) â€” no external AI APIs required.

ðŸš€ Features

âœ… Fully local LLM inference using Ollama

âœ… Semantic search over 1000+ page Spring Boot documentation

âœ… Vector storage using PostgreSQL + PGVector

âœ… HNSW indexing for fast similarity search

âœ… Cosine similarity distance metric

âœ… Automatic PDF ingestion at startup

âœ… Token-based chunking for better retrieval quality

âœ… Interactive CLI using Spring Shell

âœ… Optimized for 16GB RAM development environments

ðŸ— Architecture
Spring Boot PDF
        â†“
PagePdfDocumentReader
        â†“
TokenTextSplitter
        â†“
Embedding Model (Ollama - nomic-embed)
        â†“
PGVector (HNSW Index)
        â†“
Retriever
        â†“
LLM (Ollama - qwen2.5)
        â†“
Spring Shell CLI Response
ðŸ›  Tech Stack

Java 21

Spring Boot

Spring AI

Spring Shell

Ollama (Local LLM Runtime)

PostgreSQL

PGVector Extension

HNSW Indexing

ðŸ§  Models Used
Chat Model

qwen2.5:1.5b

Lightweight, efficient for local inference

Embedding Model

nomic-embed-fast

768-dimensional embeddings

Optimized for semantic similarity search

âš™ï¸ Setup Instructions
1ï¸âƒ£ Install Ollama
https://ollama.com/download

Pull required models:

ollama pull qwen2.5:1.5b
ollama pull nomic-embed-fast
2ï¸âƒ£ Setup PostgreSQL + PGVector

Enable pgvector extension:

CREATE EXTENSION IF NOT EXISTS vector;
3ï¸âƒ£ Configure application.yml
spring:
  shell:
    interactive:
      enabled: true

  datasource:
    url: jdbc:postgresql://localhost:5432/sbdocs
    username: admin
    password: password

  ai:
    ollama:
      base-url: http://localhost:11434
      chat:
        options:
          model: qwen2.5:1.5b
      embedding:
        model: nomic-embed-fast

    vectorstore:
      pgvector:
        initialize-schema: true
        index-type: HNSW
        distance-type: COSINE_DISTANCE
        dimensions: 768
4ï¸âƒ£ Fix Windows JLine Terminal Issue

Add VM option:

-Djline.terminal=jni

This forces native Windows terminal provider.

5ï¸âƒ£ Run Application
mvn clean install
mvn spring-boot:run
ðŸ–¥ Example Usage

Once started:

shell:> ask "What is Spring Boot auto-configuration?"

The assistant retrieves relevant documentation chunks and generates a contextual answer.

ðŸ“ˆ Performance Characteristics

Sub-second vector retrieval using HNSW

Memory usage optimized for 16GB RAM systems

Fully offline inference (no API latency)

Suitable for large technical documentation corpora

ðŸ”Ž Key Engineering Highlights

Designed end-to-end RAG pipeline

Implemented PDF ingestion + token chunking

Tuned embedding dimensions to match PGVector schema

Debugged and resolved JLine terminal provider conflicts on Windows

Migrated from external Gemini APIs to fully local Ollama stack

Optimized index type and similarity metric for documentation retrieval

ðŸ§ª Future Improvements

Streaming LLM responses

Hybrid search (BM25 + Vector)

Web-based UI interface

Multi-document support

Response citation highlighting

Dockerized deployment

ðŸŽ¯ Why This Project?

This project demonstrates:

Practical LLM integration in backend systems

Vector database engineering

Performance optimization

System architecture thinking

Real-world RAG implementation using Java ecosystem
